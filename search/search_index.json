{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Amazon EMR Train-The-Trainer Workshop \u00b6 This workshop contains exercises for the 3-day Amazon EMR Workshop Training. Agenda \u00b6 Day 1 \u00b6 Launch EMR cluster with Managed Scaling and Fleets Run Spark ETL on Amazon EMR and EMR Studio/Notebooks Build Incremental Workloads on Amazon EMR using Apache Hudi and Apache Iceberg Run Streaming Workloads on Amazon EMR Day 2 \u00b6 Orchestrate analytical workloads using EMR Notebooks API and Amazon MWAA Run Spark workload using Amazon EMR on EKS Run Spark/Hive jobs on EMR Serverless Day 3 \u00b6 Run Hive and Presto workloads on Amazon EMR Run HBase workloads on Amazon EMR Integrate with AWS LakeFormation and Apache Ranger","title":"Introduction"},{"location":"#welcome-to-amazon-emr-train-the-trainer-workshop","text":"This workshop contains exercises for the 3-day Amazon EMR Workshop Training.","title":"Welcome to Amazon EMR Train-The-Trainer Workshop"},{"location":"#agenda","text":"","title":"Agenda"},{"location":"#day-1","text":"Launch EMR cluster with Managed Scaling and Fleets Run Spark ETL on Amazon EMR and EMR Studio/Notebooks Build Incremental Workloads on Amazon EMR using Apache Hudi and Apache Iceberg Run Streaming Workloads on Amazon EMR","title":"Day 1"},{"location":"#day-2","text":"Orchestrate analytical workloads using EMR Notebooks API and Amazon MWAA Run Spark workload using Amazon EMR on EKS Run Spark/Hive jobs on EMR Serverless","title":"Day 2"},{"location":"#day-3","text":"Run Hive and Presto workloads on Amazon EMR Run HBase workloads on Amazon EMR Integrate with AWS LakeFormation and Apache Ranger","title":"Day 3"},{"location":"setup/","text":"Setup \u00b6 Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). You will see the following page. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation template \"dayone\" is created.","title":"Setup"},{"location":"setup/#setup","text":"Perform the following steps to login to the event engine. Type Event Engine URL on to your browser. (Right click this link -> Open in new tab). You will see the following page. Accept Terms & Login. Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. Once you receive OTP over your email, enter it to sign in to the Team Dashboard. Click on the SSH Key and download the key to your local desktop. Click Ok once done. Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. Go to CloudFormation and verify that the Cloudformation template \"dayone\" is created.","title":"Setup"},{"location":"day1/fleet/exercise/","text":"Exercise 1 \u2013 Launching an EMR cluster (30 mins) \u00b6 This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises. Create EMR Cluster with Instance Fleets \u00b6 Go to the EMR Web Console from AWS Management Console Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps. Step 1: Software and Steps \u00b6 Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next. Step 2: Hardware \u00b6 There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next. Step 3: General Cluster Settings \u00b6 Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next. Step 4: Security \u00b6 Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"1 - Launching EMR cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#exercise-1-launching-an-emr-cluster-30-mins","text":"This exercise is meant to show you different options and features available while trying to create an EMR cluster. EMR clusters required are already created in your event engine accounts which we will use for our exercises.","title":"Exercise 1 \u2013 Launching an EMR cluster (30 mins)"},{"location":"day1/fleet/exercise/#create-emr-cluster-with-instance-fleets","text":"Go to the EMR Web Console from AWS Management Console Click on \u201cCreate cluster\u201d Click on \u201cGo to advanced options\u201d Explore the options on all 4 Steps.","title":"Create EMR Cluster with Instance Fleets"},{"location":"day1/fleet/exercise/#step-1-software-and-steps","text":"Choose latest release label: EMR 6.5.0. Look at the applications available. Choose Spark, Hive and Hue for example. You can choose Use multiple master nodes to improve cluster availability which will launch 3 X EMR Leader Nodes. You can use AWS Glue Data Catalog for Hive and Spark tables. Under Software Configurations, you can provide a JSON config to override default values. For example, you can use below JSON: [{ \"Classification\" : \"spark\" , \"Properties\" : { \"maximizeResourceAllocation\" : \"true\" } }, { \"classification\" : \"hive-site\" , \"properties\" : { \"hive.blobstore.use.output-committer\" : \"true\" } }, { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [{ \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_DATANODE_HEAPSIZE\" : \"2048\" , \"HADOOP_NAMENODE_OPTS\" : \"-XX:GCTimeRatio=19\" }, \"Configurations\" : [ ] }] } ] Select \"Run multiple steps at the same time to improve cluster utilization\". You can change the step concurrency as well which is defaulted to 10. You can submit a Spark step during cluster creation. But for now, we can leave it as is. We can also submit steps to a running EMR cluster. For transient or short-lived EMR clusters, you can add steps during cluster creation and choose to auto-terminate your clusters after last step completion using option \"Cluster auto-terminates\". For now, we can leave it at default \"Clusters enters waiting state\". Click on Next.","title":"Step 1: Software and Steps"},{"location":"day1/fleet/exercise/#step-2-hardware","text":"There are two types of Cluster Compositions: Uniform instance groups and Instance fleets. Uniform instance groups will allow you to provision only one instance type within a single node group. Also, this option will only look at a single subnet while provisioning clusters. Let's choose instance fleets which provide us more flexibility in terms of hardware configuration. Under Networking, leave it at default VPC and choose all the 6 EC2 subnets in that VPC. Allocation strategy option is an improved method of launching clusters with lowest-priced On-Demand instances and capacity-optimized Spot instances. This option is recommended for faster cluster provisioning, more accurate Spot instance allocation, and fewer Spot instance interruptions compared to default EMR instance fleet allocation. Select \"Apply allocation strategy\". Under Cluster Nodes and Instances, for leader node type, click on \"Add / remove instance types to fleet\" and choose m5.xlarge, m4.xlarge, c5.xlarge, c4.xlarge. Choose on-demand. For core node type, choose 8 on-demand units and 8 spot units and select m5.xlarge, m5.2xlarge, m4.xlarge, m4.2xlarge. Under Provisioning timeout after \"60\" minutes Spot unavailability, change 60 mins to 30 mins and choose \"Switch to On-Demand instances\" from the drop down. For task node type, choose 8 spot units and select r5.xlarge, r5.2xlarge, r4.xlarge, r4.2xlarge. You can choose up to 30 different instance types for each node type. Click on \"Enable cluster scaling\" and choose core and task units. Choose minimum=16, maximum=32, on-demand limit=16, maximum core node=16. You can also enable scaling after the cluster has been launched. Enabling auto-termination helps you save cost by terminating idle clusters. You can leave this enabled since we will not be using this cluster. You can change the EBS root volume. You can increase this value if you are installing many different applications on your cluster. For now, you do not need to change the value. Click on Next.","title":"Step 2: Hardware"},{"location":"day1/fleet/exercise/#step-3-general-cluster-settings","text":"Choose a friendly name for your cluster. Keep logging, debugging and termination protection enabled. Add a type with key named \"type\" and value \"DEV\". You have the option to customize your EC2 AMI and specify the customized image during your cluster launch. This is especially useful for applying security patches or applying CIS/STIG compliance. For now, we can use default EC2 AMI for EMR. You can specify a custom bootstrap action to run a script on all your cluster nodes. For now, we can leave it empty. Click on Next.","title":"Step 3: General Cluster Settings"},{"location":"day1/fleet/exercise/#step-4-security","text":"Under Security Options, choose EC2 key pair \"ee-default-keypair\" which is the key pair we downloaded during event engine setup. You can define custom EMR Service IAM Role which will be used by the EMR control plane and custom EC2 IAM role to be assumed by all the nodes in your cluster. If you leave these values at default, the default IAM roles (EMR_DefaultRole and EMR_EC2_DefaultRole) will be automatically created during cluster creation. Leave it as is. You can create a new Security Configuration in your EMR Web Console and use it in the \"Security Configuration\" section. This is where you define encryption, authentication and authorization for your cluster. We will look into this in detail on Day 3. For now, you can leave it at default. You can provide custom EC2 security groups for your leader and worker node types. You can configure up to 5 security groups per node type. If you do not specify, default EC2 security groups will be automatically created for leader and worker node types. Click on \"Create Cluster\". After about 10 mins, you can observe that the EMR cluster is created and is in \"WAITING\" state. Check all the tabs to see the cluster configurations.","title":"Step 4: Security"},{"location":"day1/hudi/exercise/","text":"","title":"4 - Apache Hudi on Amazon EMR"},{"location":"day1/icebrg/exercise/","text":"","title":"5 - Apache Iceberg on Amazon EMR"},{"location":"day1/spark/exercise/","text":"Exercise 2 - Apache Spark on Amazon EMR \u00b6 Initial Setup \u00b6 SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to AWS Management Console on your browser -> Amazon EMR Console -> Hardware Tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well. Using spark-submit and spark-shell \u00b6 Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session. Investigate Spark UI \u00b6 While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". #For windows, use set instead of export export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > # Replace target with your leader node instance ID aws ssm start - session -- target i - 00785e8946 b4ff636 -- document - name AWS - StartPortForwardingSession -- parameters '{\"portNumber\":[\"18080\"], \"localPortNumber\":[\"8158\"]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan. Alternative approach - Local SSH tunneling \u00b6 Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. # Replace with your leader node public DNS ssh - i ~/ ee - default - keypair . pem - N - L 8157 : ec2 - 44 - 199 - 199 - 213. compute - 1. amazonaws . com : 8088 hadoop @ec2 - 44 - 199 - 199 - 213. compute - 1. amazonaws . com Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080). Submit Spark Work to EMR using AddSteps API \u00b6 Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console. It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. #For windows, use set instead of export export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > # Replace cluster-id value with your cluster ID aws emr add - steps -- cluster - id j - 1 HCZO7EIKLFQY -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you cannot use AWS CLI for some reason, you will find an EC2 instance called \"JumpHost\" in the EC2 Web Console. In real life scenario, you can run this command from any machine as long as your IAM user or role has IAM access to invoke EMR AddSteps API. You can connect to that instance using Session Manager and submit step to EMR cluster from that session. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the AddSteps CLI command below. Replace cluster-id value with your cluster ID. You do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. aws emr add - steps -- cluster - id j - 142 PVKGDZTTXS -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output.","title":"2 - Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#exercise-2-apache-spark-on-amazon-emr","text":"","title":"Exercise 2 - Apache Spark on Amazon EMR"},{"location":"day1/spark/exercise/#initial-setup","text":"SSH to the leader node of the EMR cluster \"EMR-Spark-Hive-Presto\". The key pair you downloaded in the Setup can be used to SSH via terminal or using an SSH client like Putty. To make it easy, ssm-agent has been installed on all EMR nodes via a bootstrap action so that you can use AWS Systems Manager to login to your client EC2 instance and EMR leader node. Go to AWS Management Console on your browser -> Amazon EMR Console -> Hardware Tab. You will see MASTER and CORE fleets. Click on the MASTER instance fleet id (looks like if-XXXXXXXXXX). Click on the EC2 instance ID (looks like i-xxxxxxxxxxx). It should take you to the EC2 management console. You will be navigated to the EC2 management console. Click on \u201cConnect\u201d and go to the tab \u201cSession Manager\u201d. Click on Connect. You will be navigated to the AWS Session Manager session. In the session, type the following commands to log in as hadoop user (default OS user for EMR). sudo su hadoop cd ~ You will use the same login method to log in to other EC2 instances in this workshop as well.","title":"Initial Setup"},{"location":"day1/spark/exercise/#using-spark-submit-and-spark-shell","text":"Once you are logged into the EMR cluster using SSH or SSM agent, type \u201cspark-shell\u201d on your EMR Leader Node. Once spark-shell opens and the Spark session object is created, run the commands below - import org.apache.spark.sql.types. { IntegerType , StringType , StructType , StructField , DoubleType } val schema = StructType ( Array ( StructField ( \"s_suppkey\" , IntegerType , true ), StructField ( \"s_name\" , StringType , true ), StructField ( \"s_address\" , StringType , true ), StructField ( \"s_nationkey\" , StringType , true ), StructField ( \"s_phone\" , StringType , true ), StructField ( \"s_acctbal\" , DoubleType , true ), StructField ( \"s_comment\" , StringType , true ))) val df = spark . read . option ( \"delimiter\" , \"|\" ) . schema ( schema ) . csv ( \"s3://redshift-downloads/TPC-H/3TB/supplier/\" ) df . show ( 5 ) val df2 = df . filter ( $ \"s_acctbal\" > lit ( 0.0 )) . withColumn ( \"randdom\" , lit ( \"insert random column\" )) df2 . show ( 5 ) You will see the results in the spark-shell session.","title":"Using spark-submit and spark-shell"},{"location":"day1/spark/exercise/#investigate-spark-ui","text":"While the Spark session is still active, you can check the Spark UI. You will need to install AWS CLI and Session Manager plugin on your local desktop to do this. You will also need to update your PATH variable if it is not done automatically following this document. Otherwise you may get the error \"SessionManagerPlugin is not found\". Replace --target with your leader node instance ID in the following command. Replace the environmental variables with the values from the Team Dashboard. For Windows, you will need to use \"set\" instead of \"export\". #For windows, use set instead of export export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > # Replace target with your leader node instance ID aws ssm start - session -- target i - 00785e8946 b4ff636 -- document - name AWS - StartPortForwardingSession -- parameters '{\"portNumber\":[\"18080\"], \"localPortNumber\":[\"8158\"]}' -- region us - east - 1 Following image shows the commands run in macOS terminal. 18080 is the Spark History Server Port and 8157 is the local port. Now open http://localhost:8158 in your browser. Click on \"Show incomplete applications\" -> App ID (for eg: application_1647720368860_0002). Check out all the tabs especially the SQL tab. Click on \"show\" (Spark action) in the SQL tab to see the query plan.","title":"Investigate Spark UI"},{"location":"day1/spark/exercise/#alternative-approach-local-ssh-tunneling","text":"Please note that with this approach, you cannot access YARN Resource Manager UI. You can access it via local port forwarding by running the following command in your local desktop's terminal or using Putty for Windows. # Replace with your leader node public DNS ssh - i ~/ ee - default - keypair . pem - N - L 8157 : ec2 - 44 - 199 - 199 - 213. compute - 1. amazonaws . com : 8088 hadoop @ec2 - 44 - 199 - 199 - 213. compute - 1. amazonaws . com Now enter http://localhost:8157 on your browser to see the Resource Manager UI. You can use this method as well to access Spark UI (replace port 8088 in the above command with port 18080).","title":"Alternative approach - Local SSH tunneling"},{"location":"day1/spark/exercise/#submit-spark-work-to-emr-using-addsteps-api","text":"Let us submit Spark work to the cluster using EMR\u2019s AddSteps API. Copy the EMR Cluster ID in the Summary Tab of your EMR cluster \"EMR-Spark-Hive-Presto\" from EMR Web Console. It looks like \u2018j-XXXXXXXXXX\u2019. You can submit steps to your EMR from your local desktop after exporting the AWS credentials in your Team Dashboard page. #For windows, use set instead of export export AWS_DEFAULT_REGION = us - east - 1 export AWS_ACCESS_KEY_ID =< redacted > export AWS_SECRET_ACCESS_KEY =< redacted > export AWS_SESSION_TOKEN =< redacted > # Replace cluster-id value with your cluster ID aws emr add - steps -- cluster - id j - 1 HCZO7EIKLFQY -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 If you cannot use AWS CLI for some reason, you will find an EC2 instance called \"JumpHost\" in the EC2 Web Console. In real life scenario, you can run this command from any machine as long as your IAM user or role has IAM access to invoke EMR AddSteps API. You can connect to that instance using Session Manager and submit step to EMR cluster from that session. Once connected, enter following commands to login as ec2-user (default OS user for EC2 instances). sudo su ec2-user cd ~ Now, run the AddSteps CLI command below. Replace cluster-id value with your cluster ID. You do not need to export any credentials since the IAM role attached to this JumpHost has all accesses required. aws emr add - steps -- cluster - id j - 142 PVKGDZTTXS -- steps Name = \"Spark Pi Job\" , Jar = command - runner . jar , Args = [ spark - submit , -- master , yarn , -- num - executors , 2 , -- class , org . apache . spark . examples . SparkPi , / usr / lib / spark / examples / jars / spark - examples . jar , 10 , - v ] -- region us - east - 1 Now, check the EMR step that was submitted to the cluster. You can look into the stdout logs to see the output.","title":"Submit Spark Work to EMR using AddSteps API"},{"location":"day1/studio/exercise/","text":"","title":"3 - Amazon EMR Studio"}]}